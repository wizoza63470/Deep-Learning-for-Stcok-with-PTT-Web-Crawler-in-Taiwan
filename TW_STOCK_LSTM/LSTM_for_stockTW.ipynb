{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (1,2,5,10,12,13,16,17,19,22,23,25,27,30,32,33,40,41,50,51,55,56,66,67,71,72,74,76,77,80,82,84,86,87,89,90,91,92,93,94,96,97,100,101,102,103,104,110,118,124,128,132,133,136,137,141,142,145,146,148,153,157,160,163,164,171,172,174,176,182,183,187,189,190,191,192,193,198,201,203,204,208,211,217,222,224,225,227,230,233,237,239,240,243,247,249,252,253,254,259,264,267,276,279,280,283,285,288,290,291,293,295,301,302,304,306,307,308,309,311,314,315,316,317,318,319,320,323,325,326,327,329,331,334,338,339,340,341,343,347,348,350,353,354,360,363,366,369,375,382,384,386,389,393,395,401,406,407,408,409,410,411,412,418,419,423,424,427,429,431,432,434,437,438,440,447,449,450,456,459,465,469,471,473,474,476,477,484,485,488,490,492,497,498,506,509,510,511,512,514,516,518,519,521,524,527,535,536,542,544,545,551,552,556,557,558,563,565,571,572,574,576,580,583,584,587,589,590,591,597,607,612,615,619,624,625,627,629,633,634,636,638,639,640,642,646,654,656,659,662,674,676,677,680,686,687,688,689,694,696,697,698,699,710,711,713,721,723,724,725,728,744,751,754,761,763,765,766,768,790,792,796,798,799,806,808,818,819,829,830,839,842,847,854,869,875,880,884,885,902,909,916,917,918,919,922,923,925,926,930,933,935,938,941,948,952,956,959,972,973,974,979,981,983,987,994,999,1005,1007,1019,1028,1029,1038,1040,1043,1045,1054,1059,1060,1065,1066,1069,1070,1072,1073,1076,1078,1080,1081,1082,1083,1084,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1109,1110,1111,1112,1113,1119,1124,1131,1136,1137,1141,1144,1151) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "close = pd.read_csv('close.csv')\n",
    "close = close.transpose()\n",
    "close = close.reindex(columns=close.columns[::-1])\n",
    "close = close.drop(index = 'Unnamed: 0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.25,\n",
       " 28.3,\n",
       " nan,\n",
       " 18.0,\n",
       " 15.2,\n",
       " 11.9,\n",
       " 2.71,\n",
       " 9.25,\n",
       " 9.05,\n",
       " 8.55,\n",
       " 11.95,\n",
       " 10.7,\n",
       " 4.08,\n",
       " 2.53,\n",
       " 8.35,\n",
       " 1.25,\n",
       " 11.9,\n",
       " 7.3,\n",
       " 14.7,\n",
       " 7.25,\n",
       " 7.05,\n",
       " 6.35,\n",
       " 5.05,\n",
       " 0.75,\n",
       " 5.65,\n",
       " 10.15,\n",
       " 10.0,\n",
       " 10.2,\n",
       " 20.2,\n",
       " 20.0,\n",
       " 10.4,\n",
       " 19.6,\n",
       " 17.6,\n",
       " nan,\n",
       " nan,\n",
       " 46.7,\n",
       " 43.0,\n",
       " 11.35,\n",
       " 11.35,\n",
       " 12.45,\n",
       " 22.8,\n",
       " 16.4,\n",
       " 10.45,\n",
       " 23.5,\n",
       " 27.0,\n",
       " 11.75,\n",
       " 11.35,\n",
       " 10.95,\n",
       " 5.4,\n",
       " 9.5,\n",
       " 3.88,\n",
       " 53.5,\n",
       " 17.1,\n",
       " 11.3,\n",
       " 11.2,\n",
       " 8.0,\n",
       " 50.5,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 18.9,\n",
       " 0.93,\n",
       " 6.9,\n",
       " 7.0,\n",
       " 4.09,\n",
       " 6.9,\n",
       " 4.28,\n",
       " 6.25,\n",
       " 4.89,\n",
       " 13.55,\n",
       " 5.7,\n",
       " 6.35,\n",
       " 13.95,\n",
       " 7.9,\n",
       " 6.65,\n",
       " 13.7,\n",
       " nan,\n",
       " 9.6,\n",
       " 9.3,\n",
       " 3.81,\n",
       " 7.45,\n",
       " 3.34,\n",
       " 5.6,\n",
       " 5.25,\n",
       " 5.6,\n",
       " 5.7,\n",
       " 3.69,\n",
       " 0.78,\n",
       " 27.5,\n",
       " 6.35,\n",
       " 6.5,\n",
       " 6.25,\n",
       " 8.85,\n",
       " 3.16,\n",
       " 5.45,\n",
       " 9.0,\n",
       " 8.5,\n",
       " 0.52,\n",
       " 7.6,\n",
       " 7.25,\n",
       " 8.8,\n",
       " 5.7,\n",
       " 11.4,\n",
       " 13.45,\n",
       " 13.15,\n",
       " 5.8,\n",
       " 23.1,\n",
       " 8.25,\n",
       " 41.9,\n",
       " 12.3,\n",
       " 24.5,\n",
       " 15.3,\n",
       " 58.5,\n",
       " 19.3,\n",
       " 11.0,\n",
       " 6.65,\n",
       " 18.2,\n",
       " 8.8,\n",
       " 12.05,\n",
       " 6.15,\n",
       " 10.65,\n",
       " 6.25,\n",
       " 11.2,\n",
       " 9.3,\n",
       " 53.0,\n",
       " 45.0,\n",
       " 31.3,\n",
       " 29.5,\n",
       " 16.7,\n",
       " 29.7,\n",
       " 8.05,\n",
       " 68.0,\n",
       " 8.35,\n",
       " 7.8,\n",
       " 42.0,\n",
       " 44.9,\n",
       " 35.5,\n",
       " 29.5,\n",
       " 7.3,\n",
       " 9.1,\n",
       " 20.2,\n",
       " 23.7,\n",
       " 65.0,\n",
       " 13.4,\n",
       " 18.2,\n",
       " 17.6,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 15.0,\n",
       " 6.3,\n",
       " 10.0,\n",
       " 14.55,\n",
       " nan,\n",
       " nan,\n",
       " 8.6,\n",
       " 9.0,\n",
       " 11.55,\n",
       " 10.75,\n",
       " 10.4,\n",
       " 2.05,\n",
       " 17.7,\n",
       " 14.7,\n",
       " 12.4,\n",
       " 13.1,\n",
       " 21.5,\n",
       " nan,\n",
       " nan,\n",
       " 11.8,\n",
       " 6.75,\n",
       " 12.0,\n",
       " 8.4,\n",
       " 7.3,\n",
       " 12.7,\n",
       " 30.5,\n",
       " 11.2,\n",
       " 8.1,\n",
       " 9.35,\n",
       " 11.65,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 7.9,\n",
       " 25.7,\n",
       " 17.3,\n",
       " 17.0,\n",
       " 16.8,\n",
       " 7.75,\n",
       " 27.0,\n",
       " 42.2,\n",
       " 9.2,\n",
       " 20.7,\n",
       " 39.6,\n",
       " 11.8,\n",
       " 70.5,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 8.55,\n",
       " 35.2,\n",
       " 15.8,\n",
       " 21.2,\n",
       " 77.0,\n",
       " 40.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 26.6,\n",
       " 0.96,\n",
       " 8.8,\n",
       " 8.65,\n",
       " nan,\n",
       " 8.1,\n",
       " 9.95,\n",
       " nan,\n",
       " 9.75,\n",
       " 22.5,\n",
       " 11.65,\n",
       " 15.8,\n",
       " 6.8,\n",
       " 17.4,\n",
       " 10.55,\n",
       " 31.9,\n",
       " 32.7,\n",
       " 25.8,\n",
       " 8.4,\n",
       " 9.8,\n",
       " 12.3,\n",
       " 19.4,\n",
       " 12.5,\n",
       " 15.1,\n",
       " 14.55,\n",
       " 31.8,\n",
       " 8.8,\n",
       " 29.7,\n",
       " 9.6,\n",
       " 22.6,\n",
       " 6.8,\n",
       " 15.2,\n",
       " 23.5,\n",
       " 8.0,\n",
       " 36.8,\n",
       " 22.7,\n",
       " 49.8,\n",
       " 23.5,\n",
       " 11.7,\n",
       " 32.4,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 34.0,\n",
       " 21.8,\n",
       " 12.9,\n",
       " 9.65,\n",
       " 42.3,\n",
       " 21.7,\n",
       " 13.2,\n",
       " 10.35,\n",
       " 13.6,\n",
       " nan,\n",
       " nan,\n",
       " 36.5,\n",
       " 45.8,\n",
       " nan,\n",
       " 12.0,\n",
       " 48.1,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 35.2,\n",
       " 6.85,\n",
       " 24.1,\n",
       " 6.8,\n",
       " nan,\n",
       " 44.2,\n",
       " 25.9,\n",
       " 17.1,\n",
       " 10.85,\n",
       " 13.05,\n",
       " 14.5,\n",
       " 18.3,\n",
       " 122.0,\n",
       " 1.29,\n",
       " 4.8,\n",
       " 12.65,\n",
       " 17.8,\n",
       " 37.3,\n",
       " 25.8,\n",
       " 7.35,\n",
       " 16.5,\n",
       " 18.8,\n",
       " 2.74,\n",
       " 45.8,\n",
       " 17.0,\n",
       " 40.3,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 9.7,\n",
       " 4.9,\n",
       " 9.9,\n",
       " 10.9,\n",
       " 17.2,\n",
       " 8.15,\n",
       " 11.1,\n",
       " 4.49,\n",
       " nan,\n",
       " 14.75,\n",
       " nan,\n",
       " 15.4,\n",
       " nan,\n",
       " 19.7,\n",
       " 52.5,\n",
       " 9.1,\n",
       " 15.4,\n",
       " 12.95,\n",
       " 16.0,\n",
       " 37.6,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 47.6,\n",
       " 58.5,\n",
       " 21.5,\n",
       " 20.9,\n",
       " 80.0,\n",
       " 19.2,\n",
       " 8.8,\n",
       " 31.8,\n",
       " 13.5,\n",
       " 13.5,\n",
       " 14.15,\n",
       " 4.42,\n",
       " 33.3,\n",
       " 17.6,\n",
       " 16.1,\n",
       " 14.4,\n",
       " 16.6,\n",
       " 13.6,\n",
       " 13.6,\n",
       " 17.4,\n",
       " 23.5,\n",
       " 11.25,\n",
       " 50.0,\n",
       " 37.0,\n",
       " 6.95,\n",
       " 42.8,\n",
       " 22.3,\n",
       " 10.3,\n",
       " 72.0,\n",
       " 8.55,\n",
       " 37.8,\n",
       " 47.5,\n",
       " 25.8,\n",
       " 27.5,\n",
       " 6.85,\n",
       " 9.7,\n",
       " 65.0,\n",
       " 56.0,\n",
       " 46.7,\n",
       " 39.2,\n",
       " 38.0,\n",
       " 67.0,\n",
       " 131.0,\n",
       " 23.2,\n",
       " 16.6,\n",
       " 16.2,\n",
       " 59.5,\n",
       " 69.0,\n",
       " 30.9,\n",
       " 37.6,\n",
       " 36.4,\n",
       " 16.8,\n",
       " 13.85,\n",
       " 24.8,\n",
       " 51.5,\n",
       " 35.3,\n",
       " 32.6,\n",
       " 57.5,\n",
       " 14.15,\n",
       " 12.5,\n",
       " 18.5,\n",
       " 29.2,\n",
       " 32.8,\n",
       " 41.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 7.15,\n",
       " 19.8,\n",
       " 15.3,\n",
       " 14.8,\n",
       " 20.3,\n",
       " 15.0,\n",
       " 7.1,\n",
       " 14.9,\n",
       " 14.6,\n",
       " 21.6,\n",
       " 11.6,\n",
       " 42.2,\n",
       " 10.05,\n",
       " 18.4,\n",
       " 10.85,\n",
       " 12.95,\n",
       " 4.14,\n",
       " 19.5,\n",
       " 17.5,\n",
       " 11.3,\n",
       " 77.0,\n",
       " 5.75,\n",
       " 34.5,\n",
       " 9.55,\n",
       " 10.15,\n",
       " 11.7,\n",
       " 18.3,\n",
       " 20.8,\n",
       " 64.5,\n",
       " 30.5,\n",
       " 16.1,\n",
       " 74.0,\n",
       " 22.5,\n",
       " 11.25,\n",
       " 264.0,\n",
       " 13.5,\n",
       " 18.2,\n",
       " nan,\n",
       " 14.05,\n",
       " 24.5,\n",
       " 38.7,\n",
       " 30.1,\n",
       " 23.1,\n",
       " 15.1,\n",
       " 29.3,\n",
       " 23.0,\n",
       " 13.95,\n",
       " 9.4,\n",
       " 19.0,\n",
       " 10.8,\n",
       " 14.7,\n",
       " 22.1,\n",
       " 12.2,\n",
       " 13.95,\n",
       " 77.0,\n",
       " 107.5,\n",
       " 17.6,\n",
       " 23.9,\n",
       " 38.5,\n",
       " 27.8,\n",
       " 9.45,\n",
       " 19.4,\n",
       " 25.3,\n",
       " 21.0,\n",
       " 20.3,\n",
       " 18.4,\n",
       " 30.5,\n",
       " 13.4,\n",
       " 23.9,\n",
       " 27.8,\n",
       " 22.9,\n",
       " 5.25,\n",
       " 6.9,\n",
       " 23.7,\n",
       " 12.4,\n",
       " 1.64,\n",
       " 95.0,\n",
       " 3.94,\n",
       " 18.8,\n",
       " 139.0,\n",
       " 30.9,\n",
       " 18.2,\n",
       " 7.5,\n",
       " 10.95,\n",
       " 3.07,\n",
       " nan,\n",
       " 5.9,\n",
       " 7.6,\n",
       " 10.7,\n",
       " 6.0,\n",
       " 5.8,\n",
       " 10.85,\n",
       " 8.95,\n",
       " 55.5,\n",
       " 0.4,\n",
       " 15.6,\n",
       " 5.35,\n",
       " 0.4,\n",
       " 3.15,\n",
       " 7.05,\n",
       " 15.8,\n",
       " 9.6,\n",
       " 24.0,\n",
       " 0.81,\n",
       " 8.2,\n",
       " 22.0,\n",
       " 0.58,\n",
       " 16.5,\n",
       " 15.6,\n",
       " 9.55,\n",
       " 26.9,\n",
       " 9.4,\n",
       " 29.2,\n",
       " 25.6,\n",
       " nan,\n",
       " 7.2,\n",
       " 26.9,\n",
       " 25.1,\n",
       " 40.9,\n",
       " 13.1,\n",
       " 9.0,\n",
       " 30.4,\n",
       " 28.7,\n",
       " 16.9,\n",
       " 8.05,\n",
       " 10.7,\n",
       " 8.3,\n",
       " 15.5,\n",
       " 27.5,\n",
       " 17.3,\n",
       " 22.8,\n",
       " 13.85,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 10.8,\n",
       " 10.6,\n",
       " 17.3,\n",
       " 12.85,\n",
       " 18.2,\n",
       " 36.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 19.5,\n",
       " 48.3,\n",
       " 19.8,\n",
       " 23.8,\n",
       " 11.2,\n",
       " 3.55,\n",
       " 9.0,\n",
       " 10.5,\n",
       " 11.35,\n",
       " 10.45,\n",
       " 19.6,\n",
       " 14.7,\n",
       " 5.15,\n",
       " 7.1,\n",
       " 19.5,\n",
       " 58.5,\n",
       " nan,\n",
       " 10.85,\n",
       " 23.3,\n",
       " 13.6,\n",
       " 10.3,\n",
       " nan,\n",
       " 1.75,\n",
       " 16.8,\n",
       " 10.8,\n",
       " 10.65,\n",
       " 11.3,\n",
       " 9.3,\n",
       " 18.7,\n",
       " 14.0,\n",
       " 20.0,\n",
       " 17.7,\n",
       " 14.15,\n",
       " 12.5,\n",
       " nan,\n",
       " 27.8,\n",
       " 31.5,\n",
       " nan,\n",
       " nan,\n",
       " 62.0,\n",
       " nan,\n",
       " nan,\n",
       " 17.4,\n",
       " nan,\n",
       " 20.5,\n",
       " 14.05,\n",
       " 22.4,\n",
       " 27.8,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 28.2,\n",
       " 12.05,\n",
       " 17.5,\n",
       " 37.0,\n",
       " nan,\n",
       " nan,\n",
       " 24.9,\n",
       " nan,\n",
       " nan,\n",
       " 16.2,\n",
       " 0.41,\n",
       " 14.95,\n",
       " 3.92,\n",
       " 11.3,\n",
       " 11.3,\n",
       " 17.1,\n",
       " 20.0,\n",
       " 12.15,\n",
       " 61.0,\n",
       " 7.0,\n",
       " 12.7,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 20.7,\n",
       " 29.5,\n",
       " 22.3,\n",
       " 13.75,\n",
       " 102.0,\n",
       " 109.5,\n",
       " 287.0,\n",
       " 55.0,\n",
       " 44.4,\n",
       " 18.1,\n",
       " 23.2,\n",
       " 15.8,\n",
       " 14.3,\n",
       " 56.0,\n",
       " 13.0,\n",
       " 38.0,\n",
       " 15.3,\n",
       " 186.0,\n",
       " 24.3,\n",
       " 4.15,\n",
       " 29.0,\n",
       " 34.1,\n",
       " 27.6,\n",
       " 10.55,\n",
       " 59.5,\n",
       " 16.4,\n",
       " 22.9,\n",
       " 11.35,\n",
       " 37.5,\n",
       " 44.3,\n",
       " 15.9,\n",
       " 21.5,\n",
       " 110.0,\n",
       " 58.5,\n",
       " 28.0,\n",
       " 19.7,\n",
       " nan,\n",
       " nan,\n",
       " 33.7,\n",
       " 13.3,\n",
       " 18.4,\n",
       " 15.8,\n",
       " 17.5,\n",
       " 37.7,\n",
       " 43.4,\n",
       " 31.6,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 16.7,\n",
       " 7.95,\n",
       " 34.3,\n",
       " 20.7,\n",
       " 25.0,\n",
       " 111.0,\n",
       " 7.0,\n",
       " 1.65,\n",
       " 10.4,\n",
       " 38.0,\n",
       " 12.6,\n",
       " 27.0,\n",
       " 8.9,\n",
       " 26.0,\n",
       " 52.5,\n",
       " 34.3,\n",
       " 32.1,\n",
       " 30.4,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 19.6,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 70.5,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 28.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 54.5,\n",
       " 22.2,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 74.5,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 64.0,\n",
       " 24.7,\n",
       " 49.3,\n",
       " 49.9,\n",
       " 17.1,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 29.7,\n",
       " nan,\n",
       " nan,\n",
       " 20.3,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 9.05,\n",
       " nan,\n",
       " nan,\n",
       " 9.45,\n",
       " nan,\n",
       " nan,\n",
       " 29.6,\n",
       " 32.2,\n",
       " nan,\n",
       " 30.3,\n",
       " nan,\n",
       " 14.0,\n",
       " 22.2,\n",
       " 62.0,\n",
       " 68.0,\n",
       " 47.4,\n",
       " 22.8,\n",
       " 28.9,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 16.5,\n",
       " nan,\n",
       " 41.6,\n",
       " 23.7,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 51.0,\n",
       " nan,\n",
       " nan,\n",
       " 63.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 31.3,\n",
       " 35.2,\n",
       " nan,\n",
       " 79.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 28.2,\n",
       " nan,\n",
       " 54.5,\n",
       " 103.5,\n",
       " nan,\n",
       " 31.2,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 53.5,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " ...]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#做x資料 大X是array\n",
    "xclose = []\n",
    "xclose_train = []\n",
    "xclose_test = []\n",
    "\n",
    "n=3649\n",
    "for times in range(3649):\n",
    "    x_min =[]\n",
    "    for i in close.loc[:,n]:\n",
    "        try:\n",
    "            data = float(i)\n",
    "        except:\n",
    "            data = float('nan')\n",
    "        x_min.append(data)\n",
    "    xclose.append(x_min)\n",
    "    n = n-1\n",
    "\n",
    "xclose_train=xclose[0:3000]\n",
    "xclose_test =xclose[3000:3600]\n",
    "\n",
    "Xclose_train = np.array(xclose_train)\n",
    "Xclose_test = np.array(xclose_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#做y資料 大Y是array\n",
    "yclose = xclose[7:3607]\n",
    "yclose_train = yclose[0:3000]\n",
    "yclose_test = yclose[3000:3600]\n",
    "\n",
    "Yclose_train = np.array(yclose_train)\n",
    "Yclose_test = np.array(yclose_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 600 3000 3000\n"
     ]
    }
   ],
   "source": [
    "print(len(xclose_test),len(yclose_test),len(xclose_train),len(yclose_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xclose_train = Xclose_train.reshape(3000,1,1156)\n",
    "Xclose_test = Xclose_test.reshape(600,1,1156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.6 , 28.8 ,   nan, ...,   nan,   nan,   nan],\n",
       "       [15.4 , 28.5 ,   nan, ...,   nan,   nan,   nan],\n",
       "       [15.5 , 28.5 ,   nan, ...,   nan,   nan,   nan],\n",
       "       ...,\n",
       "       [35.  ,   nan,   nan, ..., 13.6 , 16.7 ,  9.64],\n",
       "       [35.  ,   nan,   nan, ..., 13.75, 17.4 ,  9.62],\n",
       "       [35.05,   nan,   nan, ..., 13.75, 17.45,  9.58]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yclose_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##開始做model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, BatchNormalization, LSTM, TimeDistributed, Dropout, Input, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_54: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-dd530d372d68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1156\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1156\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_54: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(750, input_shape=(1,1156)))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(1500,activation='relu'))\n",
    "model.add(Dense(1156,activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',optimizer = 'SGD', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3000/3000 [==============================] - 6s 2ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00: 0s -\n",
      "Epoch 9/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: nan - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19d065cc860>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xclose_train, Yclose_train, batch_size=7, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
